{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92dddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4041\n",
       "SparkContext available as 'sc' (version = 3.3.1, master = local[*], app id = local-1672068160286)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/27 00:22:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "22/12/27 00:22:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types.{DataTypes, StructField}\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "import java.util.Date\r\n",
       "import java.time.format.DateTimeFormatter\r\n",
       "import org.apache.spark.sql.{functions=>F}\r\n",
       "import org.apache.spark.sql.{functions=>T}\r\n",
       "import org.apache.spark.sql.expressions\r\n",
       "import org.apache.spark.sql.expressions.Window\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// импортируем пакеты\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{DataTypes, StructField}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import java.util.Date\n",
    "import java.time.format.DateTimeFormatter\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "import org.apache.spark.sql.{functions => T}\n",
    "import org.apache.spark.sql.expressions\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0a2502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/27 00:22:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@20746338\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// создаем spark сессию\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "      .master(\"local[1]\")\n",
    "      .appName(\"Final task #4\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d289dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// импортирование дополнительных пакетов\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a067e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pDate1: String = 2020-11-01\r\n",
       "pDate2: String = 2020-11-02\r\n",
       "pDate3: String = 2020-11-03\r\n",
       "pDate4: String = 2020-11-04\r\n",
       "pDateCommon: String = 2020-11-02\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Даты, на которые будет строиться витрина\n",
    "val pDate1 = \"2020-11-01\"\n",
    "val pDate2 = \"2020-11-02\"\n",
    "val pDate3 = \"2020-11-03\"\n",
    "val pDate4 = \"2020-11-04\"\n",
    "\n",
    "// Общий параметр для запуска витрин\n",
    "// Для запуска витрин просто пишем: val pDateCommon = pDate1 или val pDateCommon = pDate2 и.т.д\n",
    "\n",
    "val pDateCommon = pDate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2877053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaClients: org.apache.spark.sql.types.StructType = StructType(StructField(ClientId,IntegerType,false),StructField(ClientName,StringType,false),StructField(Type,StringType,true),StructField(Form,StringType,true),StructField(RegisterDate,DateType,false))\r\n",
       "schemaAccounts: org.apache.spark.sql.types.StructType = StructType(StructField(AccountId,IntegerType,false),StructField(AccountNum,StringType,false),StructField(ClientId,IntegerType,false),StructField(DateOpen,DateType,false))\r\n",
       "schemaOperations: org.apache.spark.sql.types.StructType = StructType(StructField(AccountDB,IntegerType,false),StructField(AccountCR,IntegerType,false),StructField(DateOp,DateType,false),StructField(Amount,DecimalType(10,2),false),StructField(Currency,StringType,false),StructField(Comment,StringType,true))\r\n",
       "sch...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Составим схему для таблиц сданными\n",
    "// Таблица клиентов\n",
    "val schemaClients = DataTypes.createStructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"ClientId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"ClientName\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"Type\", DataTypes.StringType, true),\n",
    "      DataTypes.createStructField(\"Form\", DataTypes.StringType, true),  \n",
    "      DataTypes.createStructField(\"RegisterDate\", DataTypes.DateType, false)))\n",
    "\n",
    "//Таблица счетов\n",
    "val schemaAccounts = StructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"AccountId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"AccountNum\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"ClientId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"DateOpen\", DataTypes.DateType, false)))\n",
    "\n",
    "//Таблица операций\n",
    "val schemaOperations = StructType(Array(\n",
    "      StructField(\"AccountDB\", IntegerType, false),\n",
    "      StructField(\"AccountCR\", IntegerType, false),\n",
    "      StructField(\"DateOp\", DateType, false),\n",
    "      StructField(\"Amount\", DecimalType(10, 2), false),\n",
    "      StructField(\"Currency\",StringType, false),  \n",
    "      StructField(\"Comment\", StringType, true)))\n",
    "                                                \n",
    "//Таблица курсов валют\n",
    "val schemaRates = DataTypes.createStructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"Currency\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"Rate\", DecimalType(10, 2), false),\n",
    "      DataTypes.createStructField(\"RateDate\", DataTypes.DateType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb4f3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfClients: org.apache.spark.sql.DataFrame = [ClientId: int, ClientName: string ... 3 more fields]\r\n",
       "dfAccounts: org.apache.spark.sql.DataFrame = [AccountId: int, AccountNum: string ... 2 more fields]\r\n",
       "dfOperations: org.apache.spark.sql.DataFrame = [AccountDB: int, AccountCR: int ... 4 more fields]\r\n",
       "dfRates: org.apache.spark.sql.DataFrame = [Currency: string, Rate: decimal(10,2) ... 1 more field]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// чтение полного набора данных\n",
    "// считываем Clients.csv\n",
    "val dfClients = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaClients)\n",
    "        .load(\"data/Clients.csv\")\n",
    "\n",
    "// считываем Account.csv\n",
    "val dfAccounts = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaAccounts)\n",
    "        .load(\"data/Account.csv\")\n",
    "\n",
    "// считываем Operation.csv\n",
    "val dfOperations = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .schema(schemaOperations)\n",
    "        .load(\"data/Operation.csv\")\n",
    "\n",
    "// считываем Rate.csv\n",
    "val dfRates = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaRates)\n",
    "      .load(\"data/Rate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6329a934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccounts: org.apache.spark.sql.DataFrame = [ClientId: int, ClientName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clientsAndAccounts = dfClients.join(dfAccounts,\"ClientId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4187160e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 20000\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientsAndAccounts.orderBy(\"ClientId\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083aad48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccountsOp: org.apache.spark.sql.DataFrame = [AccountDB: int, AccountCR: int ... 14 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clientsAndAccountsOp = dfOperations\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"ClientDB\"),\n",
    "                                    $\"AccountId\".as(\"AccDB\"),\n",
    "                                    $\"AccountNum\".as(\"NumDB\"),\n",
    "                                    $\"Type\".as(\"TypeDB\"),\n",
    "                                    $\"Form\".as(\"FormDB\")\n",
    "                                   ),\n",
    "          $\"AccDB\"===$\"AccountDB\" || $\"AccDB\" === $\"AccountCR\",\"left\")\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"ClientCR\"),\n",
    "                                    $\"AccountId\".as(\"AccCR\"),\n",
    "                                    $\"AccountNum\".as(\"NumCR\"),\n",
    "                                    $\"Type\".as(\"TypeCR\"),\n",
    "                                    $\"Form\".as(\"FormCR\")\n",
    "                                   ),\n",
    "         $\"AccCR\"===$\"AccountCR\",\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c53f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error: error while loading Decimal, class file 'C:\\spark\\spark-3.3.1-bin-hadoop2\\jars\\spark-catalyst_2.12-3.3.1.jar(org/apache/spark/sql/types/Decimal.class)' is broken\r\n",
       "(class java.lang.RuntimeException/error reading Scala signature of Decimal.class: assertion failed:\r\n",
       "  Decimal$DecimalIsFractional\r\n",
       "     while compiling: <console>\r\n",
       "        during phase: globalPhase=terminal, enteringPhase=jvm\r\n",
       "     library version: version 2.12.15\r\n",
       "    compiler version: version 2.12.15\r\n",
       "  reconstructed args: -Wconf:cat=deprecation:s -Wconf:cat=deprecation:ws -Wconf:cat=feature:ws -Wconf:cat=optimizer:ws -classpath  -Yrepl-class-based -Yrepl-outdir C:\\Users\\USER\\AppData\\Local\\Temp\\tmpw0rvao28\r\n",
       "\r\n",
       "  last tree to typer: TypeTree(class Byte)\r\n",
       "       tree position: line 6 of <console>\r\n",
       "            tree tpe: Byte\r\n",
       "              symbol: (final abstract) class Byte in package scala\r\n",
       "   symbol definition: final abstract class Byte extends  (a ClassSymbol)\r\n",
       "      symbol package: scala\r\n",
       "       symbol owners: class Byte\r\n",
       "           call site: constructor $eval in object $eval in package $line18\r\n",
       "\r\n",
       "== Source file context for tree position ==\r\n",
       "\r\n",
       "     3\r\n",
       "     4 object $eval {\r\n",
       "     5   lazy val $result = res1\r\n",
       "     6   lazy val $print: _root_.java.lang.String =  {\r\n",
       "     7     $iw\r\n",
       "     8\r\n",
       "     9 val sb = new _root_.scala.StringBuilder)\r\n",
       "clientsAndAccountsOpRate: org.apache.spark.sql.DataFrame = [Currency: string, AccountDB: int ... 17 more fields]\r\n",
       "clientsAndAccountsOpRate: org.apache.spark.sql.DataFrame = [Currency: string, AccountDB: int ... 17 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cформируем временную таблицу для sql\n",
    "//dfRates.createOrReplaceTempView(\"rates\")\n",
    "// определим курс на подследнюю дату\n",
    "//var lastRate = spark.sql(s\"\"\"SELECT Currency, Rate\n",
    "//                             FROM rates\n",
    "//                             WHERE RateDate = (SELECT MAX(RateDate) FROM rates)  \n",
    "//                       \"\"\")\n",
    "\n",
    "//import org.apache.spark.sql.functions._\n",
    "\n",
    "// зададим условие для объединения\n",
    "var clientsAndAccountsOpRate = clientsAndAccountsOp.join(dfRates, \"Currency\")\n",
    "// join\n",
    "//var clientsAndAccountsOpRate = clientsAndAccountsOp\n",
    "                //.join(dfRates,rateCondition,\"left\")\n",
    "              //  .drop(dfRates.col(\"Currency\"))\n",
    "\n",
    "// cформируем результирующую таблицу, с приведением рублевых сумм\n",
    "clientsAndAccountsOpRate = clientsAndAccountsOpRate\n",
    "        .withColumn(\"AmountRUB\",round(col(\"Amount\")*col(\"Rate\"),2))\n",
    "        .where($\"DateOp\" === pDateCommon)\n",
    "                                \n",
    "clientsAndAccountsOpRate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a262e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsAndAccountsOpRate.groupBy($\"AccountDB\",$\"ClientDB\",$\"DateOp\").agg(sum($\"AmountRub\").as(\"PaymentAmt\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009d3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\")   \n",
    "//         .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "//              round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\")\n",
    "//             ).show()\n",
    "\n",
    "clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\")   \n",
    "        .agg(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\")).as(\"PaymentAmt\"),\n",
    "             sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\")).as(\"EnrollementAmt\")\n",
    "            ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8643fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\")   \n",
    "        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\")\n",
    "            ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "295d4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\")   \n",
    "        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\" && $\"NumDB\".startsWith(\"40802\"),$\"AmountRub\").otherwise(0)), 2).as(\"ClearAmt\")\n",
    "            ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05851a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "// зададим условие для маски\n",
    "//val maskString = masks.select(\"maskvalue\").filter(\"id = 1\")\n",
    "//val maskCondition = maskString\n",
    "//clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\".as(\"CutoffDt\"))   \n",
    "//        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "//             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "//            round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\"),\n",
    "//             round(sum(when($\"AccDB\" === $\"AccountCR\" && $\"NumDB\".startsWith(\"40802\"),$\"AmountRub\").otherwise(0)), 2).as(\"ClearAmt\"),\n",
    "//             round(sum(when($\"AccDB\" === $\"AccountDB\" && maskCondition,$\"AmountRub\").otherwise(0)), 2).as(\"CarsAmt\")\n",
    "//            ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7695aeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_payments: org.apache.spark.sql.DataFrame = [AccountID: int, ClientId: int ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corporate_payments = clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), \n",
    "                                                          $\"ClientDB\".as(\"ClientId\"), \n",
    "                                                          $\"DateOp\")\n",
    "        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\" && $\"NumDB\".startsWith(\"40802\"),$\"AmountRub\").otherwise(0)), 2).as(\"ClearAmt\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ef1930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-02/corporate_payments\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_payments\"\n",
    "corporate_payments.write\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", path)\n",
    "    .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46deada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_account: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AccountID: int, AccountNum: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corporate_account = corporate_payments\n",
    "    .join(clientsAndAccounts.select($\"AccountNum\", \n",
    "                                    $\"AccountID\", \n",
    "                                    $\"DateOpen\", \n",
    "                                    $\"ClientName\"), \"AccountID\")\n",
    "    .withColumn(\"TotalAmt\", round($\"PaymentAmt\" + $\"EnrollementAmt\",2))\n",
    "    .select($\"AccountID\", $\"AccountNum\", $\"DateOpen\", $\"ClientId\", $\"ClientName\", $\"TotalAmt\", $\"DateOp\")\n",
    "    .orderBy(\"AccountID\", \"DateOp\")\n",
    "\n",
    "corporate_account.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aecbd49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-02/corporate_account\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "// parquet файл - экономит место, сохраняет данные с сжатом виде\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_account\"\n",
    "corporate_account.write\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", path)\n",
    "    .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73e77646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_info: org.apache.spark.sql.DataFrame = [AccountID: int, AccountNum: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corporate_info = corporate_account\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"Clid\"),\n",
    "                                    $\"ClientName\".as(\"ClName\"),\n",
    "                                    $\"Type\",\n",
    "                                    $\"Form\",\n",
    "                                    $\"RegisterDate\"), $\"Clid\"===$\"ClientId\",\"left\")\n",
    "\n",
    "\n",
    "\n",
    "corporate_info.select($\"ClientId\",\n",
    "                      $\"ClientName\",\n",
    "                      $\"Type\",\n",
    "                      $\"Form\",\n",
    "                      $\"RegisterDate\",\n",
    "                      $\"TotalAmt\",\n",
    "                      $\"DateOp\").show(5)\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe28cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-02/corporate_info\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_info\"\n",
    "corporate_info.write\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\",path)\n",
    "    .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce4b6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
