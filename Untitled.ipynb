{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92dddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4040\n",
       "SparkContext available as 'sc' (version = 3.3.1, master = local[*], app id = local-1672923420340)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types.{DataTypes, StructField, DecimalType}\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "import java.util.Date\r\n",
       "import java.time.format.DateTimeFormatter\r\n",
       "import org.apache.spark.sql.{functions=>F}\r\n",
       "import org.apache.spark.sql.{functions=>T}\r\n",
       "import org.apache.spark.sql.expressions\r\n",
       "import org.apache.spark.sql.expressions.Window\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// импортируем пакеты\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{DataTypes, StructField, DecimalType}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import java.util.Date\n",
    "import java.time.format.DateTimeFormatter\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "import org.apache.spark.sql.{functions => T}\n",
    "import org.apache.spark.sql.expressions\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0a2502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/05 21:57:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4487025a\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// создаем spark сессию\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "                                     .master(\"local[1]\")\n",
    "                                     .appName(\"Final\")\n",
    "                                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d289dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// импортирование дополнительных пакетов\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a067e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pDate1: String = 2020-11-01\r\n",
       "pDate2: String = 2020-11-02\r\n",
       "pDate3: String = 2020-11-03\r\n",
       "pDate4: String = 2020-11-04\r\n",
       "pDateCommon: String = 2020-11-01\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Даты, на которые будет строиться витрина\n",
    "val pDate1 = \"2020-11-01\"\n",
    "val pDate2 = \"2020-11-02\"\n",
    "val pDate3 = \"2020-11-03\"\n",
    "val pDate4 = \"2020-11-04\"\n",
    "\n",
    "// Общий параметр для запуска витрин\n",
    "\n",
    "val pDateCommon = pDate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2877053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaClients: org.apache.spark.sql.types.StructType = StructType(StructField(ClientId,IntegerType,false),StructField(ClientName,StringType,false),StructField(Type,StringType,true),StructField(Form,StringType,true),StructField(RegisterDate,DateType,false))\r\n",
       "schemaAccounts: org.apache.spark.sql.types.StructType = StructType(StructField(AccountId,IntegerType,false),StructField(AccountNum,StringType,false),StructField(ClientId,IntegerType,false),StructField(DateOpen,DateType,false))\r\n",
       "schemaOperations: org.apache.spark.sql.types.StructType = StructType(StructField(AccountDB,IntegerType,false),StructField(AccountCR,IntegerType,false),StructField(DateOp,DateType,false),StructField(Amount,StringType,false),StructField(Currency,StringType,false),StructField(Comment,StringType,true))\r\n",
       "schemaRate...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Составим схему для таблиц сданными\n",
    "// Таблица клиентов\n",
    "val schemaClients = DataTypes.createStructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"ClientId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"ClientName\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"Type\", DataTypes.StringType, true),\n",
    "      DataTypes.createStructField(\"Form\", DataTypes.StringType, true),  \n",
    "      DataTypes.createStructField(\"RegisterDate\", DataTypes.DateType, false)))\n",
    "\n",
    "//Таблица счетов\n",
    "val schemaAccounts = StructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"AccountId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"AccountNum\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"ClientId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"DateOpen\", DataTypes.DateType, false)))\n",
    "\n",
    "//Таблица операций\n",
    "val schemaOperations = StructType(Array(\n",
    "      StructField(\"AccountDB\", IntegerType, false),\n",
    "      StructField(\"AccountCR\", IntegerType, false),\n",
    "      StructField(\"DateOp\", DateType, false),\n",
    "      StructField(\"Amount\", StringType, false),\n",
    "      StructField(\"Currency\", StringType, false),  \n",
    "      StructField(\"Comment\", StringType, true)))\n",
    "\n",
    "//Таблица курсов валют\n",
    "val schemaRates = DataTypes.createStructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"Currency\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"Rate\", StringType, false),\n",
    "      DataTypes.createStructField(\"RateDate\", DataTypes.DateType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb4f3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfClients: org.apache.spark.sql.DataFrame = [ClientId: int, ClientName: string ... 3 more fields]\r\n",
       "dfAccounts: org.apache.spark.sql.DataFrame = [AccountId: int, AccountNum: string ... 2 more fields]\r\n",
       "dfOperations: org.apache.spark.sql.DataFrame = [AccountDB: int, AccountCR: int ... 4 more fields]\r\n",
       "dfRates: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Currency: string, Rate: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// чтение полного набора данных\n",
    "// считываем Clients.csv\n",
    "val dfClients = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaClients)\n",
    "        .load(\"data/Clients.csv\")\n",
    "\n",
    "// считываем Account.csv\n",
    "val dfAccounts = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaAccounts)\n",
    "        .load(\"data/Account.csv\")\n",
    "\n",
    "// считываем Operation.csv\n",
    "val dfOperations = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .schema(schemaOperations)\n",
    "        .load(\"data/Operation.csv\")\n",
    "        .withColumn(\"Amount\", regexp_replace($\"Amount\", \",\", \".\"))\n",
    "\n",
    "// считываем Rate.csv\n",
    "val dfRates = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaRates)\n",
    "        .load(\"data/Rate.csv\")\n",
    "        .withColumn(\"Rate\", regexp_replace($\"Rate\", \",\", \".\"))\n",
    "        .dropDuplicates(\"Currency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6329a934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccounts: org.apache.spark.sql.DataFrame = [ClientId: int, ClientName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//объеденим таблицу клиентов с таблицей счетов\n",
    "val clientsAndAccounts = dfClients.join(dfAccounts,\"ClientId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4187160e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 20000\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientsAndAccounts.orderBy(\"ClientId\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083aad48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccountsOp: org.apache.spark.sql.DataFrame = [AccountDB: int, AccountCR: int ... 12 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// к таблице опериций добавим данные о клиентах\n",
    "val clientsAndAccountsOp = dfOperations\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"ClientDB\"),\n",
    "                                    $\"AccountId\".as(\"AccDB\"),\n",
    "                                    $\"AccountNum\".as(\"NumDB\"),\n",
    "                                    $\"Type\".as(\"TypeDB\"),\n",
    "                                    $\"Form\".as(\"FormDB\")\n",
    "                                   ),\n",
    "          $\"AccDB\"===$\"AccountDB\" || $\"AccDB\" === $\"AccountCR\",\"left\")\n",
    "    .join(clientsAndAccounts.select($\"AccountId\".as(\"AccCR\"),\n",
    "                                    $\"AccountNum\".as(\"NumCR\"),\n",
    "                                    $\"Type\".as(\"TypeCR\")),\n",
    "         $\"AccCR\"===$\"AccountCR\",\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c53f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccountsOpRate: org.apache.spark.sql.DataFrame = [Currency: string, AccountDB: int ... 15 more fields]\r\n",
       "clientsAndAccountsOpRate: org.apache.spark.sql.DataFrame = [Currency: string, AccountDB: int ... 15 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var clientsAndAccountsOpRate = clientsAndAccountsOp.join(dfRates, \"Currency\")\n",
    "\n",
    "clientsAndAccountsOpRate = clientsAndAccountsOpRate\n",
    "        .withColumn(\"AmountRUB\",round(col(\"Amount\")*col(\"Rate\"),2))\n",
    "        .where($\"DateOp\" === pDateCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7695aeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_payments: org.apache.spark.sql.DataFrame = [AccountID: int, ClientId: int ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val corporate_payments = clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), \n",
    "                                                          $\"ClientDB\".as(\"ClientId\"), \n",
    "                                                          $\"DateOp\".as(\"CutoffDt\"))\n",
    "        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\" && $\"NumDB\".startsWith(\"40802\"),$\"AmountRub\").otherwise(0)), 2).as(\"ClearAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"TypeCR\" === \"Ф\", $\"AmountRub\").otherwise(0)), 2).as(\"FLAmt\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52609fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+--------------+------+----------+-----+\n",
      "|AccountID|ClientId|  CutoffDt|PaymentAmt|EnrollementAmt|TaxAmt|  ClearAmt|FLAmt|\n",
      "+---------+--------+----------+----------+--------------+------+----------+-----+\n",
      "|     8592|    8592|2020-11-01|       0.0|    1328220.19|   0.0|1328220.19|  0.0|\n",
      "|     8932|    8932|2020-11-01|       0.0|     2005215.2|   0.0|       0.0|  0.0|\n",
      "|    12559|    2559|2020-11-01|       0.0|     1439812.6|   0.0|       0.0|  0.0|\n",
      "|     5375|    5375|2020-11-01|       0.0|     866025.62|   0.0| 866025.62|  0.0|\n",
      "|    16461|    6461|2020-11-01| 1920178.4|           0.0|   0.0|       0.0|  0.0|\n",
      "+---------+--------+----------+----------+--------------+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corporate_payments.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4ef1930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-01/corporate_payments\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_payments\"\n",
    "corporate_payments.write\n",
    "                  .format(\"parquet\")\n",
    "                  .option(\"path\", path)\n",
    "                  .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46deada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_account: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AccountID: int, AccountNum: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corporate_account = corporate_payments\n",
    "    .join(clientsAndAccounts.select($\"AccountNum\", \n",
    "                                    $\"AccountID\", \n",
    "                                    $\"DateOpen\", \n",
    "                                    $\"ClientName\"), \"AccountID\")\n",
    "    .withColumn(\"TotalAmt\", round(col(\"PaymentAmt\") + col(\"EnrollementAmt\"), 2))\n",
    "    .select($\"AccountID\", $\"AccountNum\", $\"DateOpen\", $\"ClientId\", $\"ClientName\", $\"TotalAmt\", $\"CutoffDt\")\n",
    "    .orderBy(\"AccountID\", \"CutoffDt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95a0d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corporate_account.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aecbd49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-01/corporate_account\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_account\"\n",
    "corporate_account.write\n",
    "                 .format(\"parquet\")\n",
    "                 .option(\"path\", path)\n",
    "                 .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73e77646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_info: org.apache.spark.sql.DataFrame = [AccountID: int, AccountNum: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var corporate_info = corporate_account\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"Clid\"),\n",
    "                                    $\"ClientName\".as(\"ClName\"),\n",
    "                                    $\"Type\",\n",
    "                                    $\"Form\",\n",
    "                                    $\"RegisterDate\"), $\"Clid\"===$\"ClientId\",\"left\")                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "209e21f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_info: org.apache.spark.sql.DataFrame = [ClientId: int, ClientName: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corporate_info = corporate_info.distinct()\n",
    "                                .groupBy($\"ClientId\",\n",
    "                                         $\"ClientName\",\n",
    "                                         $\"Type\",\n",
    "                                         $\"Form\",\n",
    "                                         $\"RegisterDate\",\n",
    "                                         $\"CutoffDt\")\n",
    "                                .sum(\"TotalAmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94fb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corporate_info.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fe28cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-01/corporate_info\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_info\"\n",
    "corporate_info.write\n",
    "              .format(\"parquet\")\n",
    "              .option(\"path\",path)\n",
    "              .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce4b6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
