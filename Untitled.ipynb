{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92dddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4040\n",
       "SparkContext available as 'sc' (version = 3.3.1, master = local[*], app id = local-1672408874728)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types.{DataTypes, StructField}\r\n",
       "import org.apache.spark.sql.types._\r\n",
       "import java.util.Date\r\n",
       "import java.time.format.DateTimeFormatter\r\n",
       "import org.apache.spark.sql.{functions=>F}\r\n",
       "import org.apache.spark.sql.{functions=>T}\r\n",
       "import org.apache.spark.sql.expressions\r\n",
       "import org.apache.spark.sql.expressions.Window\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// импортируем пакеты\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{DataTypes, StructField}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import java.util.Date\n",
    "import java.time.format.DateTimeFormatter\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "import org.apache.spark.sql.{functions => T}\n",
    "import org.apache.spark.sql.expressions\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0a2502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/30 23:01:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7cb2d3ae\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// создаем spark сессию\n",
    "val spark:SparkSession = SparkSession.builder()\n",
    "      .master(\"local[1]\")\n",
    "      .appName(\"Final\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d289dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// импортирование дополнительных пакетов\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a067e5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pDate1: String = 2020-11-01\r\n",
       "pDate2: String = 2020-11-02\r\n",
       "pDate3: String = 2020-11-03\r\n",
       "pDate4: String = 2020-11-04\r\n",
       "pDateCommon: String = 2020-11-01\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Даты, на которые будет строиться витрина\n",
    "val pDate1 = \"2020-11-01\"\n",
    "val pDate2 = \"2020-11-02\"\n",
    "val pDate3 = \"2020-11-03\"\n",
    "val pDate4 = \"2020-11-04\"\n",
    "\n",
    "// Общий параметр для запуска витрин\n",
    "\n",
    "val pDateCommon = pDate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2877053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaClients: org.apache.spark.sql.types.StructType = StructType(StructField(ClientId,IntegerType,false),StructField(ClientName,StringType,false),StructField(Type,StringType,true),StructField(Form,StringType,true),StructField(RegisterDate,DateType,false))\r\n",
       "schemaAccounts: org.apache.spark.sql.types.StructType = StructType(StructField(AccountId,IntegerType,false),StructField(AccountNum,StringType,false),StructField(ClientId,IntegerType,false),StructField(DateOpen,DateType,false))\r\n",
       "schemaOperations: org.apache.spark.sql.types.StructType = StructType(StructField(AccountDB,IntegerType,false),StructField(AccountCR,IntegerType,false),StructField(DateOp,DateType,false),StructField(Amount,StringType,false),StructField(Currency,StringType,false),StructField(Comment,StringType,true))\r\n",
       "schemaRate...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Составим схему для таблиц сданными\n",
    "// Таблица клиентов\n",
    "val schemaClients = DataTypes.createStructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"ClientId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"ClientName\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"Type\", DataTypes.StringType, true),\n",
    "      DataTypes.createStructField(\"Form\", DataTypes.StringType, true),  \n",
    "      DataTypes.createStructField(\"RegisterDate\", DataTypes.DateType, false)))\n",
    "\n",
    "//Таблица счетов\n",
    "val schemaAccounts = StructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"AccountId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"AccountNum\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"ClientId\", DataTypes.IntegerType, false),\n",
    "      DataTypes.createStructField(\"DateOpen\", DataTypes.DateType, false)))\n",
    "\n",
    "//Таблица операций\n",
    "val schemaOperations = StructType(Array(\n",
    "      StructField(\"AccountDB\", IntegerType, false),\n",
    "      StructField(\"AccountCR\", IntegerType, false),\n",
    "      StructField(\"DateOp\", DateType, false),\n",
    "      StructField(\"Amount\", StringType, false),\n",
    "      StructField(\"Currency\",StringType, false),  \n",
    "      StructField(\"Comment\", StringType, true)))\n",
    "\n",
    "//Таблица курсов валют\n",
    "val schemaRates = DataTypes.createStructType(Array[StructField](\n",
    "      DataTypes.createStructField(\"Currency\", DataTypes.StringType, false),\n",
    "      DataTypes.createStructField(\"Rate\", DecimalType(10, 2), false),\n",
    "      DataTypes.createStructField(\"RateDate\", DataTypes.DateType, false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb4f3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfClients: org.apache.spark.sql.DataFrame = [ClientId: int, ClientName: string ... 3 more fields]\r\n",
       "dfAccounts: org.apache.spark.sql.DataFrame = [AccountId: int, AccountNum: string ... 2 more fields]\r\n",
       "dfOperations: org.apache.spark.sql.DataFrame = [AccountDB: int, AccountCR: int ... 4 more fields]\r\n",
       "dfOperations: org.apache.spark.sql.DataFrame = [AccountDB: int, AccountCR: int ... 4 more fields]\r\n",
       "dfRates: org.apache.spark.sql.DataFrame = [Currency: string, Rate: decimal(10,2) ... 1 more field]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// чтение полного набора данных\n",
    "// считываем Clients.csv\n",
    "val dfClients = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaClients)\n",
    "        .load(\"data/Clients.csv\")\n",
    "\n",
    "// считываем Account.csv\n",
    "val dfAccounts = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaAccounts)\n",
    "        .load(\"data/Account.csv\")\n",
    "\n",
    "// считываем Operation.csv\n",
    "var dfOperations = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .option(\"dateformat\",\"yyyy-MM-dd\")\n",
    "        .schema(schemaOperations)\n",
    "        .load(\"data/Operation.csv\")\n",
    "\n",
    "dfOperations = dfOperations.withColumn(\"Amount\", regexp_replace($\"Amount\", \",\", \".\"))\n",
    "//dfOperations = dfOperations.withColumn(\"Amount\",trim(col(\"Amount\")).cast(DecimalType(10,2)))\n",
    "\n",
    "// считываем Rate.csv\n",
    "val dfRates = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"multiline\", true)\n",
    "        .option(\"sep\", \";\")\n",
    "        .option(\"enforceSchema\", true)\n",
    "        .schema(schemaRates)\n",
    "      .load(\"data/Rate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6329a934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccounts: org.apache.spark.sql.DataFrame = [ClientId: int, ClientName: string ... 6 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clientsAndAccounts = dfClients.join(dfAccounts,\"ClientId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4187160e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 20000\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientsAndAccounts.orderBy(\"ClientId\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083aad48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccountsOp: org.apache.spark.sql.DataFrame = [AccountDB: int, AccountCR: int ... 14 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clientsAndAccountsOp = dfOperations\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"ClientDB\"),\n",
    "                                    $\"AccountId\".as(\"AccDB\"),\n",
    "                                    $\"AccountNum\".as(\"NumDB\"),\n",
    "                                    $\"Type\".as(\"TypeDB\"),\n",
    "                                    $\"Form\".as(\"FormDB\")\n",
    "                                   ),\n",
    "          $\"AccDB\"===$\"AccountDB\" || $\"AccDB\" === $\"AccountCR\",\"left\")\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"ClientCR\"),\n",
    "                                    $\"AccountId\".as(\"AccCR\"),\n",
    "                                    $\"AccountNum\".as(\"NumCR\"),\n",
    "                                    $\"Type\".as(\"TypeCR\"),\n",
    "                                    $\"Form\".as(\"FormCR\")\n",
    "                                   ),\n",
    "         $\"AccCR\"===$\"AccountCR\",\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c53f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clientsAndAccountsOpRate: org.apache.spark.sql.DataFrame = [Currency: string, AccountDB: int ... 17 more fields]\r\n",
       "clientsAndAccountsOpRate: org.apache.spark.sql.DataFrame = [Currency: string, AccountDB: int ... 17 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var clientsAndAccountsOpRate = clientsAndAccountsOp.join(dfRates, \"Currency\")\n",
    "\n",
    "clientsAndAccountsOpRate = clientsAndAccountsOpRate\n",
    "        .withColumn(\"AmountRUB\",round(col(\"Amount\")*col(\"Rate\"),2))\n",
    "        .where($\"DateOp\" === pDateCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a262e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------------------+\n",
      "|AccountDB|ClientDB|    DateOp|        PaymentAmt|\n",
      "+---------+--------+----------+------------------+\n",
      "|    11581|    1580|2020-11-01|1546250.1400000001|\n",
      "|    13705|    3704|2020-11-01|        1602487.29|\n",
      "|    10581|     580|2020-11-01|         1717906.4|\n",
      "|    16461|    6461|2020-11-01|         1920178.4|\n",
      "|    15153|    5152|2020-11-01|1875183.2000000002|\n",
      "+---------+--------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientsAndAccountsOpRate.groupBy($\"AccountDB\",$\"ClientDB\",$\"DateOp\").agg(sum($\"AmountRub\").as(\"PaymentAmt\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009d3ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+------------------+\n",
      "|AccountID|ClientId|    DateOp|PaymentAmt|    EnrollementAmt|\n",
      "+---------+--------+----------+----------+------------------+\n",
      "|     8592|    8592|2020-11-01|      null|1328220.1900000002|\n",
      "|     8932|    8932|2020-11-01|      null|         2005215.2|\n",
      "|    12559|    2559|2020-11-01|      null|         1439812.6|\n",
      "|     5375|    5375|2020-11-01|      null| 866025.6199999999|\n",
      "|    16461|    6461|2020-11-01| 1920178.4|              null|\n",
      "+---------+--------+----------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\")   \n",
    "        .agg(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\")).as(\"PaymentAmt\"),\n",
    "             sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\")).as(\"EnrollementAmt\")\n",
    "            ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8643fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+--------------+------+\n",
      "|AccountID|ClientId|    DateOp|PaymentAmt|EnrollementAmt|TaxAmt|\n",
      "+---------+--------+----------+----------+--------------+------+\n",
      "|     8592|    8592|2020-11-01|       0.0|    1328220.19|   0.0|\n",
      "|     8932|    8932|2020-11-01|       0.0|     2005215.2|   0.0|\n",
      "|    12559|    2559|2020-11-01|       0.0|     1439812.6|   0.0|\n",
      "|     5375|    5375|2020-11-01|       0.0|     866025.62|   0.0|\n",
      "|    16461|    6461|2020-11-01| 1920178.4|           0.0|   0.0|\n",
      "+---------+--------+----------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\")   \n",
    "        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\")\n",
    "            ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "295d4e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+--------------+------+----------+\n",
      "|AccountID|ClientId|    DateOp|PaymentAmt|EnrollementAmt|TaxAmt|  ClearAmt|\n",
      "+---------+--------+----------+----------+--------------+------+----------+\n",
      "|     8592|    8592|2020-11-01|       0.0|    1328220.19|   0.0|1328220.19|\n",
      "|     8932|    8932|2020-11-01|       0.0|     2005215.2|   0.0|       0.0|\n",
      "|    12559|    2559|2020-11-01|       0.0|     1439812.6|   0.0|       0.0|\n",
      "|     5375|    5375|2020-11-01|       0.0|     866025.62|   0.0| 866025.62|\n",
      "|    16461|    6461|2020-11-01| 1920178.4|           0.0|   0.0|       0.0|\n",
      "+---------+--------+----------+----------+--------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), $\"ClientDB\".as(\"ClientId\"), $\"DateOp\")   \n",
    "        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\" && $\"NumDB\".startsWith(\"40802\"),$\"AmountRub\").otherwise(0)), 2).as(\"ClearAmt\")\n",
    "            ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7695aeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_payments: org.apache.spark.sql.DataFrame = [AccountID: int, ClientId: int ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val corporate_payments = clientsAndAccountsOpRate.groupBy($\"AccDB\".as(\"AccountID\"), \n",
    "                                                          $\"ClientDB\".as(\"ClientId\"), \n",
    "                                                          $\"DateOp\".as(\"CutoffDt\"))\n",
    "        .agg(round(sum(when($\"AccDB\" === $\"AccountDB\", $\"AmountRub\").otherwise(0)), 2).as(\"PaymentAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\", $\"AmountRub\").otherwise(0)), 2).as(\"EnrollementAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountDB\" && $\"NumCR\".startsWith(\"40702\"),$\"AmountRub\").otherwise(0)), 2).as(\"TaxAmt\"),\n",
    "             round(sum(when($\"AccDB\" === $\"AccountCR\" && $\"NumDB\".startsWith(\"40802\"),$\"AmountRub\").otherwise(0)), 2).as(\"ClearAmt\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f52609fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+--------------+------+----------+\n",
      "|AccountID|ClientId|  CutoffDt|PaymentAmt|EnrollementAmt|TaxAmt|  ClearAmt|\n",
      "+---------+--------+----------+----------+--------------+------+----------+\n",
      "|     8592|    8592|2020-11-01|       0.0|    1328220.19|   0.0|1328220.19|\n",
      "|     8932|    8932|2020-11-01|       0.0|     2005215.2|   0.0|       0.0|\n",
      "|    12559|    2559|2020-11-01|       0.0|     1439812.6|   0.0|       0.0|\n",
      "|     5375|    5375|2020-11-01|       0.0|     866025.62|   0.0| 866025.62|\n",
      "|    16461|    6461|2020-11-01| 1920178.4|           0.0|   0.0|       0.0|\n",
      "+---------+--------+----------+----------+--------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corporate_payments.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ef1930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-01/corporate_payments\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_payments\"\n",
    "corporate_payments.write\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", path)\n",
    "    .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46deada7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_account: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [AccountID: int, AccountNum: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corporate_account = corporate_payments\n",
    "    .join(clientsAndAccounts.select($\"AccountNum\", \n",
    "                                    $\"AccountID\", \n",
    "                                    $\"DateOpen\", \n",
    "                                    $\"ClientName\"), \"AccountID\")\n",
    "    .withColumn(\"TotalAmt\", round($\"PaymentAmt\" + $\"EnrollementAmt\",2))\n",
    "    .select($\"AccountID\", $\"AccountNum\", $\"DateOpen\", $\"ClientId\", $\"ClientName\", $\"TotalAmt\", $\"CutoffDt\")\n",
    "    .orderBy(\"AccountID\", \"CutoffDt\")\n",
    "\n",
    "corporate_account.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aecbd49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-01/corporate_account\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_account\"\n",
    "corporate_account.write\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", path)\n",
    "    .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73e77646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corporate_info: org.apache.spark.sql.DataFrame = [AccountID: int, AccountNum: string ... 10 more fields]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val corporate_info = corporate_account\n",
    "    .join(clientsAndAccounts.select($\"ClientId\".as(\"Clid\"),\n",
    "                                    $\"ClientName\".as(\"ClName\"),\n",
    "                                    $\"Type\",\n",
    "                                    $\"Form\",\n",
    "                                    $\"RegisterDate\"), $\"Clid\"===$\"ClientId\",\"left\")                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "209e21f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalStateException",
     "evalue": " Cannot call methods on a stopped SparkContext.\r",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\r",
      "This stopped SparkContext was created at:\r",
      "\r",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r",
      "py4j.Gateway.invoke(Gateway.java:238)\r",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r",
      "java.lang.Thread.run(Thread.java:750)\r",
      "\r",
      "The currently active SparkContext was created at:\r",
      "\r",
      "(No active SparkContext.)\r",
      "\r",
      "  at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\r",
      "  at org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2524)\r",
      "  at org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\r",
      "  at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\r",
      "  at org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:155)\r",
      "  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\r",
      "  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\r",
      "  at scala.collection.immutable.List.foldLeft(List.scala:91)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:154)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:530)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:491)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:487)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:487)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:487)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:487)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:487)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:521)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\r",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\r",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:521)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:235)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:230)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:372)\r",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:808)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:767)\r",
      "  ... 41 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "corporate_info.select($\"ClientId\",\n",
    "                      $\"ClientName\",\n",
    "                      $\"Type\",\n",
    "                      $\"Form\",\n",
    "                      $\"RegisterDate\",\n",
    "                      $\"TotalAmt\",\n",
    "                      $\"CutoffDt\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fe28cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = Vitrina/2020-11-01/corporate_info\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// запишем в parquet\n",
    "val path = \"Vitrina/\"+pDateCommon+\"/corporate_info\"\n",
    "corporate_info.write\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\",path)\n",
    "    .mode(\"Overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce4b6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
